{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa4ef51",
   "metadata": {},
   "source": [
    "# Rodrigo Schulz\n",
    "\n",
    "## Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d71558",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o Adaboost e o GBM\n",
    "Adaboost (AdaBoost) e Gradient Boosting Machine (GBM) são dois algoritmos populares de aprendizado de máquina que pertencem à família de algoritmos de boosting. Eles têm algumas semelhanças, mas também apresentam diferenças distintas. Aqui estão cinco diferenças entre o Adaboost e o GBM:\n",
    "\n",
    "1. **Algoritmo base:**\n",
    "   - Adaboost: Usa um único algoritmo de aprendizado fraco (geralmente árvores de decisão fracas) e treina várias cópias desse algoritmo, ajustando seus pesos em cada iteração.\n",
    "   - GBM: Utiliza um algoritmo de aprendizado fraco (também árvores de decisão fracas) e treina iterativamente várias cópias desse algoritmo, ajustando os erros residuais do modelo anterior em cada iteração.\n",
    "\n",
    "2. **Pesos das instâncias:**\n",
    "   - Adaboost: Atribui pesos às instâncias de treinamento, aumentando os pesos das instâncias classificadas incorretamente nas iterações anteriores para que recebam mais atenção nas iterações subsequentes.\n",
    "   - GBM: Não atribui pesos às instâncias, mas ajusta os pesos dos resíduos da iteração anterior para minimizar o erro global.\n",
    "\n",
    "3. **Abordagem de ajuste:**\n",
    "   - Adaboost: Ajusta os modelos de forma sequencial, dando maior ênfase aos exemplos difíceis em cada iteração.\n",
    "   - GBM: Ajusta os modelos de forma aditiva, focando na minimização dos resíduos acumulados do modelo anterior em cada iteração.\n",
    "\n",
    "4. **Convergência:**\n",
    "   - Adaboost: Pode sofrer com o problema de overfitting se o número de iterações for muito grande.\n",
    "   - GBM: Geralmente é menos suscetível ao overfitting, pois ajusta gradualmente os erros residuais.\n",
    "\n",
    "5. **Paralelismo:**\n",
    "   - Adaboost: As iterações geralmente ocorrem de forma sequencial, o que pode tornar o algoritmo menos eficiente em ambientes paralelos.\n",
    "   - GBM: Pode ser paralelizado, tornando possível treinar árvores e estimar os resíduos em paralelo, o que pode acelerar o processo de treinamento.\n",
    "\n",
    "Em resumo, Adaboost e GBM compartilham o objetivo de combinar vários modelos fracos para criar um modelo forte, mas diferem na forma como atribuem pesos às instâncias de treinamento, na abordagem de ajuste e na paralelização do processo de treinamento. Ambos os algoritmos têm suas aplicações e podem ser úteis para diferentes tipos de problemas de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43877f3f",
   "metadata": {},
   "source": [
    "### 2. Crie um exemplo de classificação e de regressão utilizando GBM no python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a029fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo GBM para classificação: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gerando dados de exemplo para classificação\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Dividindo o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando o modelo GBM para classificação\n",
    "gbm_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Treinando o modelo\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Realizando previsões no conjunto de teste\n",
    "y_pred = gbm_classifier.predict(X_test)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia do modelo GBM para classificação: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88d934e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro Médio Quadrático do modelo GBM para regressão: 2.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Gerando dados de exemplo para regressão\n",
    "X = np.random.rand(100, 1)  # Uma característica (feature) unidimensional\n",
    "y = 2 * X[:, 0] + np.random.randn(100)  # Valor alvo (target) com ruído\n",
    "\n",
    "# Dividindo o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando o modelo GBM para regressão\n",
    "gbm_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Treinando o modelo\n",
    "gbm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Realizando previsões no conjunto de teste\n",
    "y_pred = gbm_regressor.predict(X_test)\n",
    "\n",
    "# Calculando o erro médio quadrático do modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Erro Médio Quadrático do modelo GBM para regressão: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2e009",
   "metadata": {},
   "source": [
    "### 3. Cite 5 hiperparametros importantes do GBM\n",
    "\n",
    "O Gradient Boosting Machine (GBM) é um algoritmo de aprendizado de máquina baseado em árvores de decisão que requer alguns hiperparâmetros para ajustar seu comportamento durante o treinamento. Aqui estão cinco hiperparâmetros importantes do GBM:\n",
    "\n",
    "1. **Número de estimadores (n_estimators):**\n",
    "   - Especifica o número de árvores de decisão (estimadores) que serão criadas durante o processo de treinamento. Um número maior de estimadores geralmente resulta em modelos mais complexos e ajustados, mas também pode aumentar o risco de overfitting.\n",
    "\n",
    "2. **Taxa de aprendizado (learning_rate):**\n",
    "   - Controla a contribuição de cada árvore de decisão para a correção dos erros residuais dos modelos anteriores. Uma taxa de aprendizado menor implica que cada árvore tem uma correção mais suave, tornando o treinamento mais lento, mas geralmente resulta em melhores modelos.\n",
    "\n",
    "3. **Profundidade máxima das árvores (max_depth):**\n",
    "   - Define a profundidade máxima das árvores de decisão. Limitar a profundidade ajuda a evitar overfitting e torna o modelo mais simples. No entanto, uma profundidade muito pequena pode levar a um ajuste insuficiente, resultando em um desempenho pobre.\n",
    "\n",
    "4. **Número mínimo de amostras para split (min_samples_split):**\n",
    "   - Especifica o número mínimo de amostras necessárias em um nó para que uma divisão (split) seja realizada. Um valor maior ajuda a evitar divisões que resultem em folhas com poucas amostras, tornando o modelo mais robusto e reduzindo o risco de overfitting.\n",
    "\n",
    "5. **Número mínimo de amostras em cada folha (min_samples_leaf):**\n",
    "   - Define o número mínimo de amostras necessárias em uma folha (nó terminal) da árvore de decisão. Um valor maior ajuda a evitar folhas com poucas amostras, reduzindo a complexidade do modelo e evitando overfitting.\n",
    "\n",
    "Esses são alguns dos hiperparâmetros mais importantes do GBM. É essencial ajustar esses hiperparâmetros adequadamente para obter o melhor desempenho do modelo em diferentes conjuntos de dados e problemas. Geralmente, é realizada a busca em uma grade (grid search) ou otimização bayesiana para encontrar a combinação ideal de hiperparâmetros para um problema específico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08aec8",
   "metadata": {},
   "source": [
    "### 4. Utilize o Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36d8eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f134a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_ref</th>\n",
       "      <th>id_cliente</th>\n",
       "      <th>sexo</th>\n",
       "      <th>posse_de_veiculo</th>\n",
       "      <th>posse_de_imovel</th>\n",
       "      <th>qtd_filhos</th>\n",
       "      <th>tipo_renda</th>\n",
       "      <th>educacao</th>\n",
       "      <th>estado_civil</th>\n",
       "      <th>tipo_residencia</th>\n",
       "      <th>idade</th>\n",
       "      <th>tempo_emprego</th>\n",
       "      <th>qt_pessoas_residencia</th>\n",
       "      <th>renda</th>\n",
       "      <th>mau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Assalariado</td>\n",
       "      <td>Secundário</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Casa</td>\n",
       "      <td>49</td>\n",
       "      <td>8.605479</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1916.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Empresário</td>\n",
       "      <td>Secundário</td>\n",
       "      <td>União</td>\n",
       "      <td>Casa</td>\n",
       "      <td>60</td>\n",
       "      <td>6.953425</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2967.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Empresário</td>\n",
       "      <td>Secundário</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Casa</td>\n",
       "      <td>28</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>2.0</td>\n",
       "      <td>340.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Assalariado</td>\n",
       "      <td>Superior completo</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Casa</td>\n",
       "      <td>60</td>\n",
       "      <td>1.879452</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4903.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Empresário</td>\n",
       "      <td>Secundário</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Casa</td>\n",
       "      <td>47</td>\n",
       "      <td>8.438356</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3012.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_ref  id_cliente sexo  posse_de_veiculo  posse_de_imovel  qtd_filhos  \\\n",
       "0 2015-01-01           1    F              True             True           0   \n",
       "1 2015-01-01           2    M              True            False           0   \n",
       "2 2015-01-01           3    F              True            False           0   \n",
       "3 2015-01-01           4    F             False             True           0   \n",
       "4 2015-01-01           5    F             False            False           0   \n",
       "\n",
       "    tipo_renda           educacao estado_civil tipo_residencia  idade  \\\n",
       "0  Assalariado         Secundário       Casado            Casa     49   \n",
       "1   Empresário         Secundário        União            Casa     60   \n",
       "2   Empresário         Secundário       Casado            Casa     28   \n",
       "3  Assalariado  Superior completo       Casado            Casa     60   \n",
       "4   Empresário         Secundário       Casado            Casa     47   \n",
       "\n",
       "   tempo_emprego  qt_pessoas_residencia    renda  mau  \n",
       "0       8.605479                    2.0  1916.54    0  \n",
       "1       6.953425                    2.0  2967.25    0  \n",
       "2       0.682192                    2.0   340.96    0  \n",
       "3       1.879452                    2.0  4903.16    0  \n",
       "4       8.438356                    2.0  3012.60    0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('credit_scoring.csv', parse_dates=['data_ref'])\n",
    "\n",
    "df['tempo_emprego'].fillna(-1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b754feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas no treino:     42000\n",
      "Quantidade de linhas no teste:      18000\n",
      "Quantidade de linhas na validação:  15000\n"
     ]
    }
   ],
   "source": [
    "# Selecionar meses de 2016 para validação\n",
    "df_val = df[df['data_ref'] >= datetime(2016, 1, 1)].copy()\n",
    "\n",
    "# Selecionar meses de 2015 para treinamento e teste\n",
    "df = df[df['data_ref'] < datetime(2016, 1, 1)]\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=12)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "print('Quantidade de linhas no treino:    ', df_train.shape[0])\n",
    "print('Quantidade de linhas no teste:     ', df_test.shape[0])\n",
    "print('Quantidade de linhas na validação: ', df_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3d754b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "equacao = '''mau ~ sexo + posse_de_veiculo + posse_de_imovel + qtd_filhos + tipo_renda + educacao \n",
    "            + estado_civil + tipo_residencia + idade + qt_pessoas_residencia + renda'''\n",
    "\n",
    "y_train, X_train = patsy.dmatrices(equacao, data=df_train)\n",
    "y_test, X_test = patsy.dmatrices(equacao, data=df_test)\n",
    "y_val, X_val = patsy.dmatrices(equacao, data=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab0f83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o Gini\n",
    "def calcula_gini(RESP, PD):\n",
    "    #AUC\n",
    "    auc = roc_auc_score(RESP, PD)\n",
    "\n",
    "    #Gini\n",
    "    gini = 2 * auc - 1\n",
    "    return gini\n",
    "\n",
    "\n",
    "def print_metricas(dados, PD='PD', CLASSE_PRED='classe_predita', RESP='mau'):\n",
    "\n",
    "    #Acuracia\n",
    "    acc = accuracy_score(dados[RESP], dados[CLASSE_PRED])\n",
    "\n",
    "    #AUC\n",
    "    auc = roc_auc_score(dados[RESP], dados[PD])\n",
    "\n",
    "    #Gini\n",
    "    gini = 2 * auc - 1\n",
    "\n",
    "    #KS\n",
    "    ks = ks_2samp(dados.loc[dados[RESP] == 1, PD], dados.loc[dados[RESP] != 1,\n",
    "                                                             PD]).statistic\n",
    "\n",
    "    print('KS:       {0:.2f}%'.format(ks * 100))\n",
    "    print('AUC:      {0:.2f}%'.format(auc * 100))\n",
    "    print('GINI:     {0:.2f}%'.format(gini * 100))\n",
    "    print('Acurácia: {0:.2f}%\\n'.format(acc * 100))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a96b81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 20s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parametros = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': None,\n",
    "    'min_samples_leaf': 5,\n",
    "    'learning_rate': .1,\n",
    "    'random_state': 22\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(**parametros)\\\n",
    "        .fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51016940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance do GBM nos dados de treino\n",
      "KS:       97.51%\n",
      "AUC:      99.69%\n",
      "GINI:     99.39%\n",
      "Acurácia: 98.03%\n",
      "\n",
      "Performance do GBM nos dados de teste\n",
      "KS:       18.73%\n",
      "AUC:      62.02%\n",
      "GINI:     24.04%\n",
      "Acurácia: 91.05%\n",
      "\n",
      "Performance do GBM nos dados de validação\n",
      "KS:       19.54%\n",
      "AUC:      63.43%\n",
      "GINI:     26.87%\n",
      "Acurácia: 90.26%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['classe_predita'] = clf.predict(X_train)\n",
    "df_train['PD'] = clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "df_test['classe_predita'] = clf.predict(X_test)\n",
    "df_test['PD'] = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_val['classe_predita'] = clf.predict(X_val)\n",
    "df_val['PD'] = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print('Performance do GBM nos dados de treino')\n",
    "print_metricas(dados=df_train)\n",
    "print('Performance do GBM nos dados de teste')\n",
    "print_metricas(dados=df_test)\n",
    "print('Performance do GBM nos dados de validação')\n",
    "print_metricas(dados=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "parametros = {\n",
    "    'n_estimators': [100, 300, 600],\n",
    "    'min_samples_leaf': [2, 10, 20],\n",
    "    'learning_rate': [0.04, 0.06, .1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=gb,\n",
    "                    param_grid=parametros,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=False,\n",
    "                    cv=2)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416e6c4",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo de Jerome Friedman (Stochastic) e pensado no nome dado ao Stochastic GBM, qual a maior diferença entre os dois algoritmos?\n",
    "\n",
    "O termo \"Stochastic GBM\" refere-se a uma variação estocástica (ou aleatória) do algoritmo Gradient Boosting Machine (GBM). A maior diferença entre o GBM tradicional e o GBM estocástico está na forma como as árvores de decisão são construídas durante o processo de treinamento.\n",
    "\n",
    "No Gradient Boosting Machine tradicional, cada árvore de decisão é construída usando todo o conjunto de treinamento disponível, o que significa que todas as amostras do conjunto de treinamento são consideradas para cada nó da árvore durante o processo de divisão (split). As amostras não são repetidas e todas têm igual probabilidade de serem selecionadas para a construção da árvore.\n",
    "\n",
    "Por outro lado, no Stochastic GBM, cada árvore de decisão é construída usando apenas uma amostra aleatória do conjunto de treinamento. Isso significa que, em cada iteração, uma fração do conjunto de treinamento é amostrada aleatoriamente, com substituição, para construir uma árvore. Essa abordagem introduz aleatoriedade no processo de construção das árvores e é especialmente útil quando o conjunto de treinamento é muito grande.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
